name	method	nf	hamPrecision	spamPrecision	avgPrecision	hamRecall	spamRecall	avgRecall	areaUnderPRC	areaUnderROC	hamFMeasure	spamFMeasure	fMeasure	trainTime	testTime
LING_SPAM	FD	1024	98,56 ± 0,18	98,50 ± 0,12	98,53 ± 0,08	98,36 ± 0,13	98,69 ± 0,16	98,52 ± 0,08	49,91 ± 0,12	98,52 ± 0,08	98,46 ± 0,09	98,46 ± 0,09	98,53 ± 0,08	00:03:04.781 ± 00:00:12.971	00:00:00.482 ± 00:00:00.019
SPAM_ASSASSIN	MI	128	94,26 ± 0,43	97,24 ± 0,41	95,75 ± 0,20	97,10 ± 0,42	94,54 ± 0,39	95,82 ± 0,20	51,35 ± 0,35	95,82 ± 0,20	95,66 ± 0,20	95,66 ± 0,20	95,79 ± 0,20	00:00:29.906 ± 00:00:03.515	00:00:00.281 ± 00:00:00.007
SPAM_ASSASSIN	MI	256	94,44 ± 0,41	97,12 ± 0,34	95,78 ± 0,19	96,95 ± 0,35	94,74 ± 0,39	95,84 ± 0,19	51,19 ± 0,31	95,84 ± 0,19	95,68 ± 0,19	95,68 ± 0,19	95,81 ± 0,19	00:00:21.313 ± 00:00:04.857	00:00:00.325 ± 00:00:00.002
TREC	MI	1024	82,56 ± 0,11	93,45 ± 0,13	88,00 ± 0,08	94,10 ± 0,11	80,88 ± 0,11	87,49 ± 0,08	56,28 ± 0,09	87,49 ± 0,08	87,95 ± 0,08	87,95 ± 0,08	87,75 ± 0,08	05:54:10.620 ± 00:41:52.967	00:00:09.158 ± 00:00:00.258
